# -*- coding: utf-8 -*-
"""Calling_The Model_V3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qmv6qv4e5xsM02mHq88wFGamGfKsbtLo
"""

!pip install -qU \
  transformers==4.31.0 \
  sentence-transformers==2.2.2 \
  pinecone-client==2.2.2 \
  datasets==2.14.0 \
  accelerate==0.21.0 \
  einops==0.6.1 \
  langchain==0.0.240 \
  xformers==0.0.20 \
  bitsandbytes==0.41.0

!pip install -qqq loralib==0.1.1
!pip install -qU torch

# Hugging Face Token
from huggingface_hub import notebook_login

notebook_login()

# Imports
from torch import cuda, bfloat16
import transformers
from peft import (
    LoraConfig,
    PeftConfig,
    PeftModel,
    get_peft_model,
    prepare_model_for_kbit_training
)
import torch
from transformers import StoppingCriteria, StoppingCriteriaList
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.chains.conversation.memory import ConversationBufferWindowMemory

# Define Model Name
model_id = "kunalg080198/llama-13b-trained-corpuschange2"

def load_chatbot():
  """
  This function defines the bits and bytes configuration.
  It also loads the model and tokenizer from provate hugging face repo.

  """
  # Defining the Stopping Criteria
  device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

  # Bits and Bytes speccs
  bnb_config = transformers.BitsAndBytesConfig(
      load_in_4bit=True,                         # load the model in 4-bit precision.
      bnb_4bit_quant_type='nf4',                 # type of quantization to use for 4-bit weights.
      bnb_4bit_use_double_quant=True,            # use double quantization for 4-bit weights.
      bnb_4bit_compute_dtype=bfloat16            # compute dtype to use for 4-bit weights.
  )

  # Peft Config for the model
  config = PeftConfig.from_pretrained(model_id)

  global model                          # Declaring model to accessed globally
  model = transformers.AutoModelForCausalLM.from_pretrained(
      config.base_model_name_or_path,   # String that specifies the path to the base model. The base model is the model that the PEFT model is based on.
                                        # It is used to initialize the parameters of the PEFT model.
      return_dict=True,
      quantization_config=bnb_config,   # used to reduce the size of the model and improve its performance.
      device_map="auto",                # the model will be assigned to the most appropriate device
      trust_remote_code=True
  )
  # Loading Tokenizer
  global tokenizer                      # Declaring tokenizer to be accessed globally
  tokenizer= transformers.AutoTokenizer.from_pretrained(config.base_model_name_or_path)   # Create a AutoTokenizer object from the base model name or path.
  tokenizer.pad_token = tokenizer.eos_token                                               # The pad_token argument is set to the eos_token. This is the end-of-sequence token.

  # Loading the model
  model = PeftModel.from_pretrained(model, model_id)

def pipeline(model_tokenizer, model_name):
  """
  This functions defines the pipeline for the model
  It also defines the stopping criteria

  You can start conversing with Llama2-13B once you call this function.

  """
  # Creating a Stop list:
  stop_list = ['\nHuman:', '\n```\n']
  # Stopping Token IDS:
  stop_token_ids_1 = [tokenizer(x)['input_ids'] for x in stop_list]
  # Converting the ids to LONG tensors: (This is mandatory)
  stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids_1]
  # Define custom stopping criteria object
  class StopOnTokens(StoppingCriteria):
  # This function is called by the training loop to check if the training should be stopped.
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
      # Iterate over the list of stop token IDs.
      for stop_ids in stop_token_ids:
        # Check if the model predicts the stop token sequence.
        if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():
          return True
      return False  # If the model does not predict the stop token sequence, return False.

  # Loading stopping Criteria
  stopping_criteria = StoppingCriteriaList([StopOnTokens()])

  # Model Pipeline
  generate_text = transformers.pipeline(
    model=model,
    tokenizer=tokenizer,
    return_full_text=True,               # langchain expects the full text
    task='text-generation',
    # we pass model parameters here too
    stopping_criteria=stopping_criteria, # without this model goes off topic during chat after a point
    temperature=0.1,                     # 'randomness' of outputs, 0.0 is the min and 1.0 the max
    top_p=0.15,                          # select from top tokens whose probability add up to 15%, you can experiment with this
    top_k=0,                             # select from top 0 tokens (because zero, relies on top_p)
    max_new_tokens=512,                  # max number of tokens to generate in the output, this should be not high enough to generate randomness and low enough to be precise
    repetition_penalty=1.1               # without this output begins repeating
  )


  # Loading the model in LangChain
  prompt_template = """
  The following is a friendy conversation between a human and an AI based on the content provided.
  The AI is conversational and retrives answers for the questions asked and is concise in it's responses.
  If the AI does not know the answer to a question, it truthfully says it does not know.

  Current conversation:
  {chat_history}
  Human: {input}
  AI:"""

  # Defining our memoory
  memory = ConversationBufferWindowMemory(
      k=20,                              # Number of previous conversations to store
      return_only_outputs=True,
      memory_key="chat_history"         # this has align with agent prompt (below)

  )

  # Instantiate the LLM
  llm = HuggingFacePipeline(pipeline=generate_text)


  # Define your QnA retrieval chain
  chat = RetrievalQA.from_chain_type(
      llm=llm,
      chain_type="stuff",
      retriever=docsearch.as_retriever(), # This docsearch/index is where we are using Pinecone as a data retriever DB
      memory = memory,
      verbose = False)


  def chat_trim(chat_chain, query):
    """
    There are some unwanted characters in the response by the LLM at the end. This functions cleans them up
    """
    # create response
    chat_chain.run(query)
    # check for double newlines (also happens often)
    chat.memory.chat_memory.messages[-1].content = chat.memory.chat_memory.messages[-1].content.split('\n\n')[0]
    # strip any whitespace
    chat.memory.chat_memory.messages[-1].content = chat.memory.chat_memory.messages[-1].content.strip()
    return chat.memory.chat_memory.messages[-1].content


  # Interacting with the bot
  while True:
    query = input("Query: ").lower()

    if query == 'exit':                          # Do have an exit statement a
      break
    else:
      answer= print("AI Response:", chat_trim(chat, query))
      continue
      return answer

# Load the model:
load_chatbot()

# Start conversation:
pipeline(tokenizer, model)

